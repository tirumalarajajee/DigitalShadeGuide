<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Tooth Shade Detection</title>
  <style>
   body { font-family: sans-serif; text-align: center; background: #f0f0f0; }
#camera { display: none; }
#snapshot { display: block; margin: 10px auto; border: 1px solid #000; }

    #confidenceFill {
      height: 20px;
      background: gray;
      width: 100px;
      margin: auto;
      color: white;
      line-height: 20px;
    }
  </style>
</head>
<body>
<label for="cameraSelect">Camera:</label>
<select id="cameraSelect"></select>

  <h2>ðŸ¦· Detection Mode</h2>
  <video id="camera" autoplay playsinline width="640" height="480"></video>
  <canvas id="snapshot" width="640" height="480" style="border:1px solid #000;"></canvas>

  <br />

  <input type="text" id="patient" placeholder="Patient Name">
 <!-- Capture workflow -->
<label for="captureType">Capture Type:</label>
<select id="captureType">
  <option value="single">Single</option>
  <option value="region">Region-wise</option>
  <option value="merge">Merged</option>
</select>

<br><br>

<!-- Tooth region (used only for region/merge modes) -->
<h3 id="instruction">Place camera at: â€”</h3>
<button id="captureBtn">Capture</button>



  <p id="liveShade">Shade: ---</p>
  <div id="confidenceFill">Î”E00: ---</div>

  <br />
  <button id="saveSingle">Save Single</button>
  <button id="saveRegion">Save Region</button>
  <button id="saveMerge">Save Merged</button>
<script src="js/jszip.min.js"></script>


  <script>
document.addEventListener("DOMContentLoaded", () => {
  initCameraSystem();
  initCaptureWorkflow();
});
const snapshotCanvas = document.getElementById("snapshot");
const snapshotCtx = snapshotCanvas.getContext("2d");
if (!snapshotCanvas) {
  throw new Error("Snapshot canvas missing in DOM");
}

let sessionCompleted = false;


    const cameraSelect = document.getElementById("cameraSelect");
let currentStream = null;

async function populateCameraSelector() {
  const devices = await navigator.mediaDevices.enumerateDevices();
  const cameras = devices.filter(d => d.kind === "videoinput");

  cameraSelect.innerHTML = "";

  cameras.forEach(cam => {
    const option = document.createElement("option");
    option.value = cam.deviceId;
    option.text = cam.label || "Camera";
    cameraSelect.appendChild(option);
  });

  // Restore saved camera if exists
  const savedCameraId = localStorage.getItem("preferredCameraId");
  if (savedCameraId) {
    cameraSelect.value = savedCameraId;
  }
}

async function startCameraById(deviceId) {
  if (currentStream) {
    currentStream.getTracks().forEach(t => t.stop());
  }

  const stream = await navigator.mediaDevices.getUserMedia({
    video: {
      deviceId: deviceId ? { exact: deviceId } : undefined,
      width: { ideal: 640 },
      height: { ideal: 480 }
    }
  });

  currentStream = stream;
  video.srcObject = stream;
}
cameraSelect.onchange = async () => {
  const id = cameraSelect.value;
  localStorage.setItem("preferredCameraId", id);
  console.log("[Camera] User selected:", id);
  await startCameraById(id);
};
async function initCameraSystem() {
  await populateCameraSelector();

  const savedCameraId = localStorage.getItem("preferredCameraId");
  if (savedCameraId) {
    console.log("[Camera] Restoring saved camera");
    await startCameraById(savedCameraId);
  } else {
    console.log("[Camera] No saved camera, using default");
    await startCameraById(null);
  }
}

function speak(text) {
  if (!speechUnlocked) return;
  const u = new SpeechSynthesisUtterance(text);
  speechSynthesis.cancel();
  speechSynthesis.speak(u);
}


const CAPTURE_WORKFLOWS = {
  single: ["center"],

  region: ["incisal", "middle", "cervical"],

  merge: ["mesial", "distal"]
};


let session = {
  patient: null,
  type: null,
  steps: [],
  currentIndex: 0,
  captures: []
};
document.getElementById("captureBtn").onclick = () => {

  if (sessionCompleted) {
    console.log("[Session] Previous session completed. Waiting for user.");
    return;
  }

  if (!session.steps.length) {
    const captureType = document.getElementById("captureType").value;
    const patientName = document.getElementById("patient").value;
    startSession(captureType, patientName);
  }

  handleCapture();
};



function initCaptureWorkflow() {
  const captureTypeEl = document.getElementById("captureType");
  const patientEl = document.getElementById("patient");

  if (!captureTypeEl) {
    console.error("[Init] captureType element not found");
    return;
  }

  if (!patientEl) {
    console.error("[Init] patient input not found");
    return;
  }

  captureTypeEl.addEventListener("change", () => {
    startSession(captureTypeEl.value, patientEl.value);
  });
}

function startSession(captureType, patientName) {
  sessionCompleted = false;

  session.patient = patientName.trim();
  session.type = captureType;
  session.steps = CAPTURE_WORKFLOWS[captureType];
  session.currentIndex = 0;
  session.captures = [];

  updateInstruction();
  speakInstruction();
}


function updateInstruction() {
  const step = session.steps[session.currentIndex];
  const textMap = {
    center: "Center of the tooth",
    incisal: "Incisal third",
    middle: "Middle third",
    cervical: "Cervical third",
    mesial: "Mesial tooth",
    distal: "Distal tooth"
  };

  document.getElementById("instruction").innerText =
    `Place camera at: ${textMap[step]}`;
}
function speakInstruction() {
  const step = session.steps[session.currentIndex];

  const speechMap = {
    center: "Place camera at the center of the tooth",
    incisal: "Place camera at incisal third",
    middle: "Incisal captured. Move sensor to middle third",
    cervical: "Middle captured. Move sensor to cervical third",
    mesial: "Place camera at mesial tooth",
    distal: "Mesial captured. Move sensor to distal tooth"
  };

  const text = speechMap[step];
  if (text) speak(text);
}

async function handleCapture() {
  const region = session.steps[session.currentIndex];

  speak(`${region} captured`);

  const result = await captureFrameAndAnalyze(region);
  session.captures.push(result);

  session.currentIndex++;

  if (session.currentIndex < session.steps.length) {
    updateInstruction();
    speakInstruction();
  } else {
    speak("All regions captured. Saving file.");
    finalizeSession();
  }
}



async function captureFrameAndAnalyze(region) {
 if (!snapshotCanvas || !snapshotCtx) {
    console.error("[Capture] Snapshot canvas not available");
    return null;
  }

  snapshotCtx.drawImage(video, 80, 40, 160, 160, 0, 0, 160, 160);

  const imageData = snapshotCtx.getImageData(0, 0, 160, 160).data;

  let r=0,g=0,b=0,c=0;
  for (let i=0;i<imageData.length;i+=4) {
    r+=imageData[i]; g+=imageData[i+1]; b+=imageData[i+2]; c++;
  }
  r/=c; g/=c; b/=c;

  const [shade, delta] = findClosestShade([r,g,b]);

  const imageBlob = await new Promise(res => canvas.toBlob(res, "image/jpeg"));
  const base64 = await blobToBase64(imageBlob);

  return {
    region,
    shade,
    delta: delta.toFixed(2),
    image: base64
  };
}
function blobToBase64(blob) {
  return new Promise(res => {
    const reader = new FileReader();
    reader.onloadend = () => res(reader.result.split(",")[1]);
    reader.readAsDataURL(blob);
  });
}

function downloadJSON(filename, data) {
  console.log("[Download] JSON:", filename);

  const blob = new Blob(
    [JSON.stringify(data, null, 2)],
    { type: "application/json" }
  );

  const url = URL.createObjectURL(blob);
  const a = document.createElement("a");
  a.href = url;
  a.download = filename;
  document.body.appendChild(a);
  a.click();

  setTimeout(() => {
    URL.revokeObjectURL(url);
    a.remove();
  }, 100);
}


function finalizeSession() {
  const timestamp = new Date().toISOString().replace(/[:]/g, "-");

  sessionCompleted = true;

  if (session.type === "single") {
    const record = {
      patient: session.patient,
      type: "single",
      timestamp,
      shade: session.captures[0].shade,
      delta: session.captures[0].delta
    };

    downloadJSON(`${session.patient}_${timestamp}.json`, record);
    resetSession();
    return;
  }

  exportZipSession(timestamp);
}


function exportZipSession(timestamp) {
  const zip = new JSZip();
  const root = zip.folder(
    `${session.patient}_${session.type}_${timestamp}`
  );

  session.captures.forEach(c => {
    root.file(`${c.region}.jpg`, c.image, { base64: true });
  });

  root.file(
    "result.json",
    JSON.stringify({
      patient: session.patient,
      type: session.type,
      timestamp,
      captures: session.captures
    }, null, 2)
  );

  zip.generateAsync({ type: "blob" }).then(blob => {
    const a = document.createElement("a");
    a.href = URL.createObjectURL(blob);
    a.download = `${session.patient}_${timestamp}.zip`;
    a.click();
  });

  resetSession();
}
function resetSession() {
  session = {
    patient: null,
    type: null,
    steps: [],
    currentIndex: 0,
    captures: []
  };

  document.getElementById("instruction").innerText =
    "Session complete. Select capture type to start again.";

  console.log("[Session] Reset complete");
}



let speechUnlocked = false;

document.body.addEventListener("click", () => {
  if (!speechUnlocked && "speechSynthesis" in window) {
    const u = new SpeechSynthesisUtterance(" ");
    speechSynthesis.speak(u);
    speechUnlocked = true;
    console.log("[Speech] Speech synthesis unlocked by user gesture");
  }
}, { once: true });




let lastSpokenShade = null;
let lastSpokenTime = 0;
const SPEAK_INTERVAL_MS = 3000; // minimum gap between announcements


   const DB_NAME = "DigitalShadeDB";
const DB_VERSION = 1;
const SHADE_STORE = "shades";
const META_STORE = "meta";





function speakShade(shade, delta) {
  console.log("[Speech] speakShade called:", shade, "Î”", delta.toFixed(2));

  if (!speechUnlocked) {
    console.warn("[Speech] Blocked: speech not unlocked yet");
    return;
  }

  if (!("speechSynthesis" in window)) {
    console.error("[Speech] Web Speech API not supported");
    return;
  }

  if (!shade || shade === "---") {
    console.warn("[Speech] Invalid shade, skipping");
    return;
  }

  if (shade === lastSpokenShade) {
    console.log("[Speech] Same shade as last, skipping");
    return;
  }

  const utterance = new SpeechSynthesisUtterance(shade);

  utterance.rate = 0.9;
  utterance.pitch = 1.0;
  utterance.volume = 1.0;

  speechSynthesis.cancel();
  speechSynthesis.speak(utterance);

  lastSpokenShade = shade;

  console.log("[Speech] Speaking shade:", shade);
}



let webShades = {}; // will be populated from IndexedDB

function openDB() {
  return new Promise((resolve, reject) => {
    const req = indexedDB.open(DB_NAME, DB_VERSION);
    req.onupgradeneeded = e => {
      const db = e.target.result;
      if (!db.objectStoreNames.contains(SHADE_STORE))
        db.createObjectStore(SHADE_STORE, { keyPath: "shadeCode" });
      if (!db.objectStoreNames.contains(META_STORE))
        db.createObjectStore(META_STORE, { keyPath: "key" });
    };
    req.onsuccess = () => resolve(req.result);
    req.onerror = () => reject(req.error);
  });
}

async function getStoredVersion(db) {
  return new Promise(res => {
    const tx = db.transaction(META_STORE, "readonly");
    const req = tx.objectStore(META_STORE).get("version");
    req.onsuccess = () => res(req.result?.value || null);
  });
}

async function syncShades() {
  console.log("[ShadeDB] Sync started");

  const db = await openDB();
  const json = await fetch("shades.json").then(r => r.json());

  console.log("[ShadeDB] JSON version:", json.version);

  const storedVersion = await getStoredVersion(db);
  console.log("[ShadeDB] Cached version:", storedVersion);

  if (storedVersion === json.version) {
    console.log("[ShadeDB] Using cached shades (IndexedDB)");
    await loadShadesFromDB(db);
    return;
  }

  console.log(`[ShadeDB] Updating cache: ${storedVersion} â†’ ${json.version}`);

  const tx = db.transaction([SHADE_STORE, META_STORE], "readwrite");
  const shadeStore = tx.objectStore(SHADE_STORE);

  shadeStore.clear();
  console.log("[ShadeDB] Cleared old shade records");

  for (const [shadeCode, rgb] of Object.entries(json.shades)) {
    shadeStore.put({ shadeCode, rgb });
  }

  tx.objectStore(META_STORE).put({
    key: "version",
    value: json.version
  });

  await tx.complete;

  console.log("[ShadeDB] Cache update complete");
  await loadShadesFromDB(db);
}


async function loadShadesFromDB(db) {
  webShades = {};
  let count = 0;

  return new Promise(resolve => {
    const tx = db.transaction(SHADE_STORE, "readonly");
    const req = tx.objectStore(SHADE_STORE).openCursor();

    req.onsuccess = e => {
      const cursor = e.target.result;
      if (cursor) {
        webShades[cursor.value.shadeCode] = cursor.value.rgb;
        count++;
        cursor.continue();
      } else {
        console.log(`[ShadeDB] Loaded ${count} shades from cache`);
        resolve();
      }
    };
  });
}

    const regions = {
      region: ["Incisal", "Middle", "Cervical"],
      merge: ["Mesial", "Prime", "Distal"]
    };

    let regionIndex = 0;
    const video = document.getElementById("camera");
    const canvas = document.getElementById("snapshot");
    const ctx = canvas.getContext("2d");
    const modeSelector = document.getElementById("modeSelector");
    const shadeLive = document.getElementById("liveShade");
    const confidenceFill = document.getElementById("confidenceFill");

    function getRegionLabel(mode) {
      if (mode === "single") return "Whole";
      const list = regions[mode];
      return list[regionIndex++ % list.length];
    }

    function getAverageRGB(data) {
      const pixels = [];
      for (let i = 0; i < data.length; i += 4) {
        pixels.push([data[i], data[i + 1], data[i + 2]]);
      }
      return pixels.reduce((a, b) => a.map((v, i) => v + b[i]), [0, 0, 0])
                   .map(v => v / pixels.length);
    }

    function rgbToLab([r, g, b]) {
      [r, g, b] = [r, g, b].map(v => {
        v /= 255;
        return v > 0.04045 ? Math.pow((v + 0.055) / 1.055, 2.4) : v / 12.92;
      });
      let x = (r * 0.4124 + g * 0.3576 + b * 0.1805) / 0.95047;
      let y = (r * 0.2126 + g * 0.7152 + b * 0.0722);
      let z = (r * 0.0193 + g * 0.1192 + b * 0.9505) / 1.08883;
      [x, y, z] = [x, y, z].map(v => v > 0.008856 ? Math.pow(v, 1/3) : (7.787 * v) + 16/116);
      return [(116 * y) - 16, 500 * (x - y), 200 * (y - z)];
    }

    function findClosestShade(rgb) {
      if (!Object.keys(webShades).length) {
    console.warn("[ShadeDB] Shade map not loaded yet");
    return ["---", Infinity];
  }
      const sampleLab = rgbToLab(rgb);
      let minDelta = Infinity, bestShade = "---";
      for (const shade in webShades) {
        const lab = rgbToLab(webShades[shade]);
        const delta = Math.sqrt(sampleLab.reduce((sum, v, i) => sum + Math.pow(v - lab[i], 2), 0));
        if (delta < minDelta) {
          minDelta = delta;
          bestShade = shade;
        }
      }
       speakShade(bestShade, minDelta);
      return [bestShade, minDelta];
    }

    setInterval(() => {
  if (!currentStream) return;

  ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

  const radius = 80;
  const data = ctx.getImageData(
    canvas.width / 2 - radius,
    canvas.height / 2 - radius,
    radius * 2,
    radius * 2
  ).data;

  const avg = getAverageRGB(data);
  const [shade, delta] = findClosestShade(avg);
  const region = session.steps[session.currentIndex] || "---";

  shadeLive.textContent = `Shade: ${shade} | Region: ${region}`;
  confidenceFill.textContent = `Î”E00: ${delta.toFixed(2)}`;
  confidenceFill.style.backgroundColor =
    delta < 3 ? "green" : delta < 6 ? "orange" : "red";
}, 150);

    function saveModeSnapshot(mode) {
      const snapshotURL = canvas.toDataURL("image/png");
      const a = document.createElement("a");
      a.href = snapshotURL;
      a.download = `${mode}_shade_${Date.now()}.png`;
      a.click();
    }

    document.getElementById("saveSingle").onclick = () => saveModeSnapshot("single");
    document.getElementById("saveRegion").onclick = () => saveModeSnapshot("region");
    document.getElementById("saveMerge").onclick = () => saveModeSnapshot("merge");

    initCameraSystem();



    syncShades();

  </script>
</body>
</html>
